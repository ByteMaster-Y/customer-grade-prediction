import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, accuracy_score

"""
pseudo_labeling_clustering_randomforest ì‹¤í—˜ì—ì„œëŠ” ì •í™•ë„ëŠ” ë†’ì•˜ì§€ë§Œ, ë¶„ë¥˜ëœ ì´ë¯¸ì§€ì—ì„œ ë°ì´í„° ê°’ë“¤ì´ í•œë° ë­‰ì³ ì œëŒ€ë¡œ ë¶„ë¦¬ë˜ì§€ ì•ŠëŠ” ë‹¨ì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.
ì •í™•ë„ì™€ ì‹œê°ì ìœ¼ë¡œ ì˜ ë¶„ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ê³ ë¯¼í•˜ê²Œ ë˜ì—ˆê³ , ì´ì— ë”°ë¼ ì½”ë“œë¥¼ í•œ ë²ˆ ë” ê°œì„ í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.
ê·¸ ê²°ê³¼, ì •í™•ë„ëŠ” 0.74ë¡œ ë‹¤ì†Œ ë‚®ì•„ì¡Œì§€ë§Œ, ì‹œê°í™”ëœ ë¶„ë¥˜ ê²°ê³¼ëŠ” í›¨ì”¬ ë” ì˜ ë¶„ë¦¬ëœ ëª¨ìŠµìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

# ---------------------------------------------------------
# 1. ì†Œìœµ ë°ì´í„° ìƒì„± (10,000ëª…, 3ê°œì˜ íŠ¹ì„±, 3ê°œ í´ë˜ìŠ¤)
# ---------------------------------------------------------
X_soyung, y_soyung = make_classification(n_samples=10000, n_features=3, 
                                         n_informative=3, n_redundant=0,
                                         n_classes=3, random_state=42)

df_soyung = pd.DataFrame(X_soyung, columns=["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "])
df_soyung["ë“±ê¸‰"] = y_soyung

# ---------------------------------------------------------
# 2. ì‚¬ê·¼ ë°ì´í„° ì¤€ë¹„ (ì†Œìœµ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ 2,000ëª… ì¼ë¶€ ë³µì‚¬)
#    - ë¼ë²¨ ì œê±° (ì§€ë„í•™ìŠµì— ì‚¬ìš© ë¶ˆê°€)
# ---------------------------------------------------------
X_sagun = X_soyung[8000:]
df_sagun = pd.DataFrame(X_sagun[:, :2], columns=["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„"])

# ---------------------------------------------------------
# 3. ì‚¬ê·¼ ë°ì´í„°ì— ë°˜í’ˆë¥  ì»¬ëŸ¼ ì¶”ê°€ (0ìœ¼ë¡œ ì±„ì›€)
# ---------------------------------------------------------
df_sagun["ë°˜í’ˆë¥ "] = 0

# ---------------------------------------------------------
# 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
#    - ì†Œìœµ ë°ì´í„°ë¡œ fit í›„, ì†Œìœµê³¼ ì‚¬ê·¼ ë‘˜ ë‹¤ ë™ì¼í•˜ê²Œ transform
# ---------------------------------------------------------
scaler = StandardScaler()
df_soyung[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]] = scaler.fit_transform(df_soyung[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]])
df_sagun[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]] = scaler.transform(df_sagun[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]])

# ---------------------------------------------------------
# 5. ì‚¬ê·¼ ë°ì´í„°ì— í´ëŸ¬ìŠ¤í„°ë§ ì ìš© (Gaussian Mixture)
# ---------------------------------------------------------
gmm = GaussianMixture(n_components=3, random_state=42)
sagun_cluster = gmm.fit_predict(df_sagun[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„"]])
df_sagun["ì˜ˆì¸¡ë“±ê¸‰"] = sagun_cluster

# ---------------------------------------------------------
# 6. ì†Œìœµ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì„ì‹œ ëª¨ë¸ í•™ìŠµ í›„, ì‚¬ê·¼ ë°ì´í„°ì— ëŒ€í•œ confidence ê³„ì‚°
# ---------------------------------------------------------
X_temp = df_soyung[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]]
y_temp = df_soyung["ë“±ê¸‰"]

temp_model = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)
temp_model.fit(X_temp, y_temp)

# ì‚¬ê·¼ ë°ì´í„° ì „ì²´ í”¼ì²˜ ì¶”ì¶œ
X_sagun_full = df_sagun[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]]
sagun_probs = temp_model.predict_proba(X_sagun_full)
sagun_confidence = np.max(sagun_probs, axis=1)
df_sagun["confidence"] = sagun_confidence

# ---------------------------------------------------------
# 7. confidence ë†’ì€ pseudo-labelë§Œ ì¶”ì¶œ (ì„ê³„ì¹˜: 0.9)
# ---------------------------------------------------------
df_sagun_filtered = df_sagun[df_sagun["confidence"] > 0.9].copy()
df_sagun_filtered["ë“±ê¸‰"] = df_sagun_filtered["ì˜ˆì¸¡ë“±ê¸‰"]

# ---------------------------------------------------------
# 8. ì†Œìœµ + ì‚¬ê·¼(pseudo) ë°ì´í„° ê²°í•© í›„ í•™ìŠµìš© ë°ì´í„° êµ¬ì„±
# ---------------------------------------------------------
df_combined = pd.concat([df_soyung, df_sagun_filtered], ignore_index=True)

X = df_combined[["ì—°ê°„_êµ¬ë§¤", "êµ¬ë§¤_ë¹ˆë„", "ë°˜í’ˆë¥ "]]
y = df_combined["ë“±ê¸‰"]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# ---------------------------------------------------------
# 9. ìµœì¢… ëª¨ë¸ í•™ìŠµ (Logistic Regression)
# ---------------------------------------------------------
final_model = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)
final_model.fit(X_train, y_train)

y_pred = final_model.predict(X_test)

print("ğŸ“Š Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# ---------------------------------------------------------
# 10. PCA ì‹œê°í™” (2D)
# ---------------------------------------------------------
# plt.rcParams['font.family'] = 'Malgun Gothic'
X_test_pca = PCA(n_components=2).fit_transform(X_test)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
df_vis = pd.DataFrame(X_test_pca, columns=["PCA1", "PCA2"])
df_vis["ì˜ˆì¸¡ë“±ê¸‰"] = y_pred

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_vis, x="PCA1", y="PCA2", hue="ì˜ˆì¸¡ë“±ê¸‰", palette="Set2", s=80)
plt.title("PCA ê¸°ë°˜ ì˜ˆì¸¡ ë“±ê¸‰ ë¶„í¬ ì‹œê°í™”", fontsize=14)
plt.tight_layout()
plt.show()
